import yt_dlp
from yt_dlp.utils import download_range_func
import os
import json
import shutil
import subprocess
from tqdm import tqdm
from multiprocessing import Pool
import time
import random
import boto3
import pandas as pd
from collections import defaultdict

from gaudio_yt_videos_list.utils.id import get_channel_or_playlist_id

# ê¸°ë³¸ ì„¤ì •
FAILED_LOG = "failed_ids_clip.txt"
UPLOAD_FAILED_LOG = "upload_failed_ids.txt"
COMPLETED_LOG = "complete_clip_ids.txt"
DOWNLOAD_DIR = "/mnt/hdd8tb/downloads_clip"
CSV_PATH = "gaudio_yt_videos_list/channel_and_playlist/final_result.csv"
TXT_DIR = 'gaudio_yt_videos_list/ytids'

S3_BUCKET = "maclab-youtube-crawl"
S3_PREFIX = "minhee_crawling"
NUM_WORKERS = 8

s3 = boto3.client("s3")

def load_failed_ids():
    if os.path.exists(FAILED_LOG):
        with open(FAILED_LOG, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    return set()

def load_completed_ids():
    if os.path.exists(COMPLETED_LOG):
        with open(COMPLETED_LOG, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    return set()

def log_failed(clip_id, error_msg=""):
    with open(FAILED_LOG, "a", encoding="utf-8") as f:
        f.write(f"{clip_id}\n")
    print(f"[ERROR] {clip_id} ì‹¤íŒ¨ ê¸°ë¡ë¨. ì‚¬ìœ : {error_msg}")

def log_completed(clip_id):
    with open(COMPLETED_LOG, "a", encoding="utf-8") as f:
        f.write(f"{clip_id}\n")

def log_upload_failed(clip_id):
    with open(UPLOAD_FAILED_LOG, "a", encoding="utf-8") as f:
        f.write(f"{clip_id}\n")

def extract_audio(mp4_path, mp3_path):
    cmd = [
        "ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
        "-i", mp4_path,
        "-vn", "-acodec", "libmp3lame", "-ab", "192k",
        mp3_path
    ]
    subprocess.run(cmd, check=True)

def s3_complete_clip_exists(clip_id):
    """
    S3ì— clip_id í´ë”ê°€ ì¡´ì¬í•˜ê³ , mp4, mp3, json íŒŒì¼ì´ ëª¨ë‘ ìˆì„ ê²½ìš° True
    ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ False (ì¦‰, ë®ì–´ì“°ê¸° ëŒ€ìƒ)
    """
    prefix = f"{S3_PREFIX}/{clip_id}/"
    required_exts = {".mp4", ".mp3", ".json"}

    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=S3_BUCKET, Prefix=prefix)

    existing_exts = set()
    for page in pages:
        for obj in page.get("Contents", []):
            key = obj["Key"]
            _, filename = key.rsplit("/", 1)
            _, ext = os.path.splitext(filename)
            existing_exts.add(ext.lower())

    return required_exts.issubset(existing_exts)

def upload_to_s3(local_path, s3_key):
    try:
        s3.upload_file(local_path, S3_BUCKET, s3_key)
        return True
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {s3_key}, ì‚¬ìœ : {e}")
        return False

def upload_clip_folder(clip_id):
    local_dir = os.path.join(DOWNLOAD_DIR, clip_id)
    if not os.path.exists(local_dir):
        return False

    # âœ… S3ì— ì™„ì „í•œ í´ë¦½ì´ ì¡´ì¬í•˜ë©´ ìŠ¤í‚µ
    if s3_complete_clip_exists(clip_id):
        print(f"ğŸš« S3ì— ì™„ì „í•œ í´ë¦½ì´ ì´ë¯¸ ì¡´ì¬í•¨ â†’ ìŠ¤í‚µ: {clip_id}")
        log_completed(clip_id)  # âœ… ëˆ„ë½ ë°©ì§€!
        return True

    print(f"â« ì—…ë¡œë“œ ì‹œì‘: {clip_id}")
    success = True
    for fname in os.listdir(local_dir):
        local_path = os.path.join(local_dir, fname)
        s3_key = f"{S3_PREFIX}/{clip_id}/{fname}"
        if not upload_to_s3(local_path, s3_key):
            success = False

    if not success:
        log_upload_failed(clip_id)

    return success

def download_and_upload(video_info):
    video_id = video_info['video_id']
    clip_id = video_info['clip_id']

    # # We already checked this in the main function
    # failed_ids = load_failed_ids()
    # completed_ids = load_completed_ids()
    # if clip_id in failed_ids or clip_id in completed_ids:
    #     return False

    video_dir = os.path.join(DOWNLOAD_DIR, clip_id)

    if os.path.exists(video_dir):
        shutil.rmtree(video_dir)
    os.makedirs(video_dir, exist_ok=True)

    mp4_path_template = os.path.join(video_dir, f"{clip_id}.%(ext)s")
    mp4_path = os.path.join(video_dir, f"{clip_id}.mp4")
    mp3_path = os.path.join(video_dir, f"{clip_id}_audio.mp3")
    json_path = os.path.join(video_dir, f"{clip_id}.info.json")

    # start_frame, end_frame = video_info['clip_start_end_idx']
    # fps = video_info['video_fps']
    # start_sec = start_frame / fps
    # end_sec = end_frame / fps

    try:
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'noplaylist': True,
            'ignoreerrors': True,
            'cookiefile': './cookies.txt',
            'outtmpl': mp4_path_template,
            'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4',
            'merge_output_format': 'mp4',
            'writeinfojson': True,
            'force_keyframes_at_cuts': True,
            # 'download_ranges': download_range_func(None, [(start_sec, end_sec)]),
            'postprocessors': [],
        }
        
        # âœ… ëœë¤í•œ ì‹œê°„ ì§€ì—° ì¶”ê°€ (0.5ì´ˆ ~ 1.5ì´ˆ)
        sleep_time = random.uniform(0.5, 1.5)
        print(f"[WAIT] {clip_id} ë‹¤ìš´ë¡œë“œ ì „ ëŒ€ê¸° ì¤‘... ({sleep_time:.2f}ì´ˆ)")
        time.sleep(sleep_time)

        # print(f">>> {clip_id} ë‹¤ìš´ë¡œë“œ ì¤‘... ({start_sec:.2f}s ~ {end_sec:.2f}s)")
        print(f">>> {clip_id} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([f"https://www.youtube.com/watch?v={video_id}"])

        if os.path.exists(mp4_path):
            extract_audio(mp4_path, mp3_path)
            
        if not (os.path.exists(mp4_path) and os.path.exists(mp3_path) and os.path.exists(json_path)):
            log_failed(clip_id, "ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì—†ìŒ")
            if os.path.exists(video_dir):
                shutil.rmtree(video_dir)
            return False

        # âœ… S3 ì—…ë¡œë“œ
        if upload_clip_folder(clip_id):
            shutil.rmtree(video_dir)
            log_completed(clip_id)
            print(f"ì—…ë¡œë“œ ì„±ê³µ: {clip_id}")
            return True
        else:
            print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {clip_id}")
            return False

    except Exception as e:
        error_msg = str(e).lower()
        log_failed(clip_id, error_msg)

        # ì¼ë°˜ ì‹¤íŒ¨ ì‹œ í´ë¦°ì—…
        if os.path.exists(video_dir):
            shutil.rmtree(video_dir)
        return False
    
def get_video_ids_per_category():
    df = pd.read_csv(CSV_PATH)
    categories_result = []
    video_ids_result = []
    
    for _, row in df.iterrows():
        # get id of channel or playlist
        channel_pl_id = get_channel_or_playlist_id(row)
        
        # read the txt file and get the video ids
        txt_file_path = os.path.join(TXT_DIR, f"{channel_pl_id}.txt")
        if os.path.exists(txt_file_path):
            with open(txt_file_path, "r", encoding="utf-8") as f:
                video_links = [line.strip() for line in f.readlines()]
            video_ids = [link.split("https://www.youtube.com/watch?v=")[-1] for link in video_links]
            
            categories_result.extend([row['category']] * len(video_ids))
            video_ids_result.extend(video_ids)
        else:
            print(f"âŒ {txt_file_path} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
    
    # create a dataframe
    result_df = pd.DataFrame({
        'category': categories_result,
        'video_id': video_ids_result
    })
    # remove duplicates
    result_df = result_df.drop_duplicates(subset=['category', 'video_id'])
    # # Check for duplicated video_id
    # duplicated = result_df[result_df.duplicated(subset=['video_id'], keep=False)]
    # if not duplicated.empty:
    #     print("âš ï¸ ì¤‘ë³µëœ video_id ë°œê²¬:")
    #     print(duplicated[['category', 'video_id']])
    # create clip_id
    result_df['clip_id'] = result_df['video_id'] + '_tmp'
    
    return result_df

def main():
    video_ids_df = get_video_ids_per_category()
    clip_ids = video_ids_df['clip_id'].tolist()
    # remove duplicates
    # data
    clip_ids = list(set(clip_ids))

    failed_ids = load_failed_ids()
    completed_ids = load_completed_ids()
    data = [clip_id for clip_id in clip_ids if clip_id not in failed_ids and clip_id not in completed_ids]

    print(f"ğŸ” ì²˜ë¦¬í•  clip_id ìˆ˜: {len(data)}")

    with Pool(NUM_WORKERS) as pool:
        with tqdm(total=len(data), desc="ë‹¤ìš´ë¡œë“œ ë° ì—…ë¡œë“œ ì§„í–‰") as pbar:
            for result in pool.imap_unordered(download_and_upload, data):
                pbar.update(1)

if __name__ == '__main__':
    main()